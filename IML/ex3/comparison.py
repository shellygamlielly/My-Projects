import matplotlib.pyplot as plt
from models import *
# from ex3.models import *


def draw_points(m):
    """

    :param m: an integer
    :return: returns a pair X,y where X is
            2 * m matrix where each column represents an i.i.d sample from the distribution D,
            and y is its corresponding label, according to f(x).
    """
    # a two-dimensional Gaussian with mean vector of zeros and a unit matrix for covariance.
    mean_vec = np.zeros(2, )
    cov = np.eye(2, dtype=int)
    X = np.random.multivariate_normal(mean_vec, cov, m)
    w = np.array([0.3, -0.5])
    y = np.sign(np.dot(X, w) + 0.1)
    return X, y


def Q9():
    """
    For each m, draw m training points and create the following figure:
    The drawn data points, colored according to their labels (e.g., blue for positive labels
    and orange for negative ones)
    Add the hyperplane of the true hypothesis (the function f)
    Add the hyperplane of the hypothesis generated by the perceptron
    Add the hyperplane of the hypothesis generated by SVM
    Add a legend to explain which hyperplane is which
    :return:
    """
    M = [5, 10, 15, 25, 70]
    for m in M:
        plt.title("Drawn data points and hypothesis for m = " + str(m))
        plt.xlabel("x1")
        plt.ylabel("x2")
        X, y = draw_points(m)
        while 0 in X or len(np.unique(y)) == 1:
            X, y = draw_points(m)

        x1 = X[y == -1]
        x2 = X[y == 1]
        plt.scatter(x1[:, 0], x1[:, 1], s=10, label="Y = -1")
        plt.scatter(x2[:, 0], x2[:, 1], s=10, label="Y = 1")

        xx = np.linspace(np.min(X), np.max(X))

        # true hypothesis
        w=[0.3,-0.5]
        b=0.1
        yy = (- w[0] * xx - b) / (w[1])

        # perceptron hypothesis
        perceptron = Perceptron()
        perceptron.fit(X, y)
        perceptron_w = perceptron.model
        w=perceptron_w[0:2]
        b=perceptron_w[2]
        yy_perceptron = ( -w[0] * xx - b) / (w[1])

        # SVC hypothesis
        svm = SVM()
        svm.fit(X, y)
        w = svm.model.coef_[0]
        a = -w[0] / w[1]
        yy_svm = a * xx - (svm.model.intercept_[0]) / w[1]

        plt.plot(xx, yy, label="True hypothesis")
        plt.plot(xx, yy_perceptron, label="Perceptron hypothesis")
        plt.plot(xx, yy_svm, label="SVC hypothesis")
        plt.legend()
        plt.show()


def create_set(m):
    """
    Draw m training points from the distribution D and classify them
    according to a true hypothesis f .
    The training data should always have points from two classes. So if you draw a training set where
    no point has yi = 1 or no point has yi = -1 then just draw a new dataset instead,
    until you get points from both types.
    Draw k test points from the same distribution D and calculate their true labels as well.
    :param m: number of samples
    :return: m training points and their labels, k testing point and their labels.
    """
    k = 10000
    trainX, trainY = draw_points(m)
    testX, testY = draw_points(k)
    while 0 in trainX or len(np.unique(trainY)) == 1:
        trainX, trainY = draw_points(m)
    while 0 in testX or len(np.unique(testY)) == 1:
        testX, testY = draw_points(m)
    return trainX, trainY, testX, testY


def calc_classifier_acc(trainX, trainY, testX, testY, classifier):
    """
    Train a Perceptron classifier, an SVM classifier and an LDA classifier on training set.
    Calculate the accuracy of these classifiers (the fraction of test points that is classified
    correctly) on the test set.
    :param trainX: training set
    :param trainY: training set labels
    :param testX: test set
    :param testY: test set labels
    :param classifier: the classifier to calculate the accuracy on
    :return: The accuracy of the classifier
    """
    #Takes too much time
    # dict = classifier.score(testX,testY)
    # acc = dict['accuracy']
    classifier.fit(trainX, trainY)
    y_pred = classifier.predict(testX)
    correct_classification = np.array(testY - y_pred == 0).sum()
    number_of_samples = testX.shape[0]
    acc = correct_classification / number_of_samples
    return acc


def calc_mean_acc(trainX, trainY, testX, testY, svm, perceptron, lda):
    """
    This function calculate the mean accuracy. For each m , repeat the procedure 500 times with k = 10000
    and save the accuracies (or just keep the mean accuracy, remember that the accuracy is a
    number between 0 to 1) of each classifier.
    :param trainX: training set
    :param trainY: training set labels
    :param testX: test set
    :param testY: test set labels
    :param svm: SVM
    :param perceptron: Perceptron
    :param lda: LDA
    :return: the mean accuracy of 500 iterations.
    """
    acc_perceptron = []
    acc_svm = []
    acc_lda = []
    for iter in range(500):
        acc_perceptron.append(calc_classifier_acc(trainX, trainY, testX, testY, perceptron))
        acc_svm.append(calc_classifier_acc(trainX, trainY, testX, testY, svm))
        acc_lda.append(calc_classifier_acc(trainX, trainY, testX, testY, lda))
    mean_acc_perceptron = np.mean(acc_perceptron)
    mean_acc_svm = np.mean(acc_svm)
    mean_acc_lda = np.mean(acc_lda)
    return mean_acc_perceptron, mean_acc_svm, mean_acc_lda


def plot_Q10():
    """
    Plot the mean accuracy as function of m for each of the algorithms (SVM, Perceptron and LDA).
    :return:
    """
    M = [5, 10, 15, 25, 70]
    acc_lda = []
    acc_perceptron = []
    acc_svm = []
    perceptron = Perceptron()
    svm = SVM()
    lda = LDA()
    for m in M:
        trainX, trainY, testX, testY = create_set(m)
        mean_acc_perceptron, mean_acc_svm, mean_acc_lda = calc_mean_acc(trainX, trainY, testX, testY, svm, perceptron,
                                                                        lda)
        acc_perceptron.append(mean_acc_perceptron)
        acc_svm.append(mean_acc_svm)
        acc_lda.append(mean_acc_lda)
    plt.title("The accuracy of each classifier as function of m" )
    plt.ylabel("Classifier Accuracy")
    plt.xlabel("m")
    plt.plot(M, acc_lda, label="LDA classifier")
    plt.plot(M, acc_perceptron, label="Perceptron classifier")
    plt.plot(M, acc_svm, label="SVC classifier")
    plt.legend()
    plt.show()


Q9()
plot_Q10()
